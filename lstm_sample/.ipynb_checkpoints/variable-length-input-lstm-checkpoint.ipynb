{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc6f83a35edff3108a58a85aa4ee04bbac5b5f2a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3674b88bde0ba98a351c739e72443d2c5da0f4fe"
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "with open(\"../input/reviews.txt\", \"r\") as lines:\n",
    "    for line in lines:\n",
    "        reviews.append(re.sub('\\s+', ' ', line).strip())\n",
    "    \n",
    "with open(\"../input/labels.txt\", \"r\") as lines:\n",
    "    for line in lines:\n",
    "        labels.append(re.sub('\\s+', ' ', line).strip())\n",
    "        \n",
    "print(reviews[-1])\n",
    "print(labels[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7dbac1c7f8121adc2b479c6c45cc9710f540204"
   },
   "outputs": [],
   "source": [
    "# now let's create and save the dictionary to use later\n",
    "char_counter = Counter()\n",
    "\n",
    "for review in reviews:\n",
    "    char_counter.update(list(review))\n",
    "    \n",
    "character_keys = {e[0]: idx+1 for idx, e in enumerate(char_counter.most_common())}\n",
    "print(character_keys)\n",
    "\n",
    "label_keys = {'positive': 0, 'negative': 1}\n",
    "print(label_keys)\n",
    "\n",
    "with open('char_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(character_keys, file)\n",
    "    \n",
    "with open('label_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(label_keys, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0713396f3eef6807c33fc8c24227423f38ff336e"
   },
   "outputs": [],
   "source": [
    "# now we can load the dicts whenever we want\n",
    "\n",
    "with open('char_dict.pkl', 'rb') as file:\n",
    "    character_keys = pickle.load(file)\n",
    "    \n",
    "with open('label_dict.pkl', 'rb') as file:\n",
    "    label_keys = pickle.load(file)\n",
    "\n",
    "print(character_keys)\n",
    "print(label_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86cd1a13bebfb6e470ebf2953fe60a7f691c530e"
   },
   "outputs": [],
   "source": [
    "# let's then tokenize our reviews and our labels\n",
    "reviews_tk = []\n",
    "labels_tk = []\n",
    "\n",
    "for review in reviews:\n",
    "    tk = [character_keys[char] for char in list(review)]\n",
    "    reviews_tk.append(tk)\n",
    "    \n",
    "labels_tk = [label_keys[label] for label in labels]\n",
    "\n",
    "print(reviews_tk[-1])\n",
    "print(labels_tk[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e4a3e567bf3ab48fa2140c6c34f11b55f0323ef"
   },
   "outputs": [],
   "source": [
    "# sort reviews by sequence length (no longer needed)\n",
    "# labels_tk = [l for r, l in sorted(zip(reviews_tk, labels_tk), key=lambda pair: -len(pair[0]))]\n",
    "# reviews_tk = sorted(reviews_tk, key=lambda r: -len(r))\n",
    "\n",
    "# for review, label in zip(reviews_tk[:15], labels_tk[:15]):\n",
    "#     print(len(review), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3645ad0a408e751d34b67cc7cd07348242b3d329"
   },
   "outputs": [],
   "source": [
    "# now we padd the reviews\n",
    "max_length = 2000\n",
    "reviews_lengths = []\n",
    "\n",
    "for idx, review in enumerate(reviews_tk):\n",
    "    review = review[:max_length]\n",
    "    reviews_lengths.append(len(review))\n",
    "    review = ([0] * (max_length - len(review))) + review\n",
    "    reviews_tk[idx] = review\n",
    "    \n",
    "for review, label, lengths in zip(reviews_tk[-15:], labels_tk[-15:], reviews_lengths[-15:]):\n",
    "    print(len(review), lengths, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "099e10d804765fa6d19ea45ccc2b41d7ad2d992d"
   },
   "outputs": [],
   "source": [
    "# make training, validation and test splits\n",
    "train_count = 20000\n",
    "val_test_count = 2500\n",
    "\n",
    "train_x = np.array(reviews_tk[:train_count])\n",
    "train_y = np.array(labels_tk[:train_count])\n",
    "\n",
    "val_x = np.array(reviews_tk[train_count:train_count+val_test_count])\n",
    "val_y = np.array(labels_tk[train_count:train_count+val_test_count])\n",
    "\n",
    "test_x = np.array(reviews_tk[-val_test_count:])\n",
    "test_y = np.array(labels_tk[-val_test_count:])\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83653737bd105c61f223f3552fa5b1faaba77453"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size=128\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd5ce9eed59b6a1a4a27484926ba9d821282b0fc"
   },
   "outputs": [],
   "source": [
    "# let's now convert the tokenized lists to torch Tensors (no longer necessary)\n",
    "\n",
    "# reviews_tk = torch.from_numpy(np.array(reviews_tk))\n",
    "# labels_tk = torch.from_numpy(np.array(labels_tk))\n",
    "    \n",
    "# print(reviews_tk[-1][-10:]) # only prints the last 10 chars, but the tensor is very big, as big as the one shown above\n",
    "# print(labels_tk[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8250aa32ff44b1ae4f7675217a0cc8bd4dd1ba2f"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "vocab_size = len(character_keys) + 1 # the + 1 is to account for the 0 that is used for padding\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, 256, padding_idx=0)\n",
    "        self.cnn1 = nn.Conv1d(256, 256, 3, padding=1, stride=2)\n",
    "        self.rnn = nn.GRU(256, 512, batch_first=True, num_layers=2, dropout=0.5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(512, 2)\n",
    "        \n",
    "        self.hidden = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        convolved = F.relu(self.cnn1(embeddings.transpose(1, 2)).transpose(2, 1))\n",
    "        # embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        lstm_out, self.hidden = self.rnn(convolved, self.hidden)\n",
    "        # lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out[:,-1,:]\n",
    "        out = torch.log_softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f52e8a0d9b67fc1ba341df5a00d1c71ff51a7c1"
   },
   "outputs": [],
   "source": [
    "model = SentimentModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15ebf1e860673c0313c01a951c732ef1499de922"
   },
   "outputs": [],
   "source": [
    "# let's train our model for 1 epoch\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model.cuda()\n",
    "for e in range(1, epochs+1):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    model.train()\n",
    "    batch = 0\n",
    "    for reviews, labels in train_loader:\n",
    "        batch += 1\n",
    "        reviews, labels = reviews.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = None\n",
    "        \n",
    "        pred = model(reviews)\n",
    "        loss = criterion(pred, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 4)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n",
    "        total_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            \n",
    "        print(f\"EPOCH {e} ({batch}/{len(train_loader)}) - loss {total_loss/batch:.4f} - acc {total_accuracy/batch:.4f}\", end='\\r') \n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for reviews, labels in valid_loader:\n",
    "            reviews, labels = reviews.cuda(), labels.cuda()\n",
    "            model.hidden = None\n",
    "            \n",
    "            pred = model(reviews)\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n",
    "            valid_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "    \n",
    "    print(f\"EPOCH {e} - loss {total_loss/len(train_loader):.4f} - acc {total_accuracy/len(train_loader):.4f} - val_loss {valid_loss/len(valid_loader):.4f} - val_acc {valid_accuracy/len(valid_loader):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbf5501a1ba8506859a671c78dccaa2c1bea4e7f"
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "model.cuda()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for reviews, labels in test_loader:\n",
    "        reviews, labels = reviews.cuda(), labels.cuda()\n",
    "        model.hidden = None\n",
    "\n",
    "        pred = model(reviews)\n",
    "        loss = criterion(pred, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n",
    "        test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "print(f\"Final model -> loss: {test_loss / len(test_loader):.4f} - accuracy: {test_accuracy / len(test_loader):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4e2013735cebc1740c3386e5c98e594462b0685"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4de7397ece95160a99108d1e379d5af7c22e15df"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
