{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc6f83a35edff3108a58a85aa4ee04bbac5b5f2a"
      },
      "cell_type": "code",
      "source": "import re\nfrom collections import Counter\nimport pickle\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3674b88bde0ba98a351c739e72443d2c5da0f4fe"
      },
      "cell_type": "code",
      "source": "reviews = []\nlabels = []\n\nwith open(\"../input/reviews.txt\", \"r\") as lines:\n    for line in lines:\n        reviews.append(re.sub('\\s+', ' ', line).strip())\n    \nwith open(\"../input/labels.txt\", \"r\") as lines:\n    for line in lines:\n        labels.append(re.sub('\\s+', ' ', line).strip())\n        \nprint(reviews[-1])\nprint(labels[-1])",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "this is one of the dumbest films i ve ever seen . it rips off nearly ever type of thriller and manages to make a mess of them all . br br there s not a single good line or character in the whole mess . if there was a plot it was an afterthought and as far as acting goes there s nothing good to say so ill say nothing . i honestly cant understand how this type of nonsense gets produced and actually released does somebody somewhere not at some stage think oh my god this really is a load of shite and call it a day . its crap like this that has people downloading illegally the trailer looks like a completely different film at least if you have download it you haven t wasted your time or money don t waste your time this is painful .\nnegative\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7dbac1c7f8121adc2b479c6c45cc9710f540204"
      },
      "cell_type": "code",
      "source": "# now let's create and save the dictionary to use later\nchar_counter = Counter()\n\nfor review in reviews:\n    char_counter.update(list(review))\n    \ncharacter_keys = {e[0]: idx+1 for idx, e in enumerate(char_counter.most_common())}\nprint(character_keys)\n\nlabel_keys = {'positive': 0, 'negative': 1}\nprint(label_keys)\n\nwith open('char_dict.pkl', 'wb') as file:\n    pickle.dump(character_keys, file)\n    \nwith open('label_dict.pkl', 'wb') as file:\n    pickle.dump(label_keys, file)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{' ': 1, 'e': 2, 't': 3, 'a': 4, 'i': 5, 'o': 6, 's': 7, 'n': 8, 'r': 9, 'h': 10, 'l': 11, 'd': 12, 'c': 13, 'm': 14, 'u': 15, 'f': 16, 'g': 17, 'y': 18, 'b': 19, 'w': 20, 'p': 21, '.': 22, 'v': 23, 'k': 24, 'j': 25, 'x': 26, 'z': 27, 'q': 28}\n{'positive': 0, 'negative': 1}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0713396f3eef6807c33fc8c24227423f38ff336e"
      },
      "cell_type": "code",
      "source": "# now we can load the dicts whenever we want\n\nwith open('char_dict.pkl', 'rb') as file:\n    character_keys = pickle.load(file)\n    \nwith open('label_dict.pkl', 'rb') as file:\n    label_keys = pickle.load(file)\n\nprint(character_keys)\nprint(label_keys)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{' ': 1, 'e': 2, 't': 3, 'a': 4, 'i': 5, 'o': 6, 's': 7, 'n': 8, 'r': 9, 'h': 10, 'l': 11, 'd': 12, 'c': 13, 'm': 14, 'u': 15, 'f': 16, 'g': 17, 'y': 18, 'b': 19, 'w': 20, 'p': 21, '.': 22, 'v': 23, 'k': 24, 'j': 25, 'x': 26, 'z': 27, 'q': 28}\n{'positive': 0, 'negative': 1}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86cd1a13bebfb6e470ebf2953fe60a7f691c530e"
      },
      "cell_type": "code",
      "source": "# let's then tokenize our reviews and our labels\nreviews_tk = []\nlabels_tk = []\n\nfor review in reviews:\n    tk = [character_keys[char] for char in list(review)]\n    reviews_tk.append(tk)\n    \nlabels_tk = [label_keys[label] for label in labels]\n\nprint(reviews_tk[-1])\nprint(labels_tk[-1])",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[3, 10, 5, 7, 1, 5, 7, 1, 6, 8, 2, 1, 6, 16, 1, 3, 10, 2, 1, 12, 15, 14, 19, 2, 7, 3, 1, 16, 5, 11, 14, 7, 1, 5, 1, 23, 2, 1, 2, 23, 2, 9, 1, 7, 2, 2, 8, 1, 22, 1, 5, 3, 1, 9, 5, 21, 7, 1, 6, 16, 16, 1, 8, 2, 4, 9, 11, 18, 1, 2, 23, 2, 9, 1, 3, 18, 21, 2, 1, 6, 16, 1, 3, 10, 9, 5, 11, 11, 2, 9, 1, 4, 8, 12, 1, 14, 4, 8, 4, 17, 2, 7, 1, 3, 6, 1, 14, 4, 24, 2, 1, 4, 1, 14, 2, 7, 7, 1, 6, 16, 1, 3, 10, 2, 14, 1, 4, 11, 11, 1, 22, 1, 19, 9, 1, 19, 9, 1, 3, 10, 2, 9, 2, 1, 7, 1, 8, 6, 3, 1, 4, 1, 7, 5, 8, 17, 11, 2, 1, 17, 6, 6, 12, 1, 11, 5, 8, 2, 1, 6, 9, 1, 13, 10, 4, 9, 4, 13, 3, 2, 9, 1, 5, 8, 1, 3, 10, 2, 1, 20, 10, 6, 11, 2, 1, 14, 2, 7, 7, 1, 22, 1, 5, 16, 1, 3, 10, 2, 9, 2, 1, 20, 4, 7, 1, 4, 1, 21, 11, 6, 3, 1, 5, 3, 1, 20, 4, 7, 1, 4, 8, 1, 4, 16, 3, 2, 9, 3, 10, 6, 15, 17, 10, 3, 1, 4, 8, 12, 1, 4, 7, 1, 16, 4, 9, 1, 4, 7, 1, 4, 13, 3, 5, 8, 17, 1, 17, 6, 2, 7, 1, 3, 10, 2, 9, 2, 1, 7, 1, 8, 6, 3, 10, 5, 8, 17, 1, 17, 6, 6, 12, 1, 3, 6, 1, 7, 4, 18, 1, 7, 6, 1, 5, 11, 11, 1, 7, 4, 18, 1, 8, 6, 3, 10, 5, 8, 17, 1, 22, 1, 5, 1, 10, 6, 8, 2, 7, 3, 11, 18, 1, 13, 4, 8, 3, 1, 15, 8, 12, 2, 9, 7, 3, 4, 8, 12, 1, 10, 6, 20, 1, 3, 10, 5, 7, 1, 3, 18, 21, 2, 1, 6, 16, 1, 8, 6, 8, 7, 2, 8, 7, 2, 1, 17, 2, 3, 7, 1, 21, 9, 6, 12, 15, 13, 2, 12, 1, 4, 8, 12, 1, 4, 13, 3, 15, 4, 11, 11, 18, 1, 9, 2, 11, 2, 4, 7, 2, 12, 1, 12, 6, 2, 7, 1, 7, 6, 14, 2, 19, 6, 12, 18, 1, 7, 6, 14, 2, 20, 10, 2, 9, 2, 1, 8, 6, 3, 1, 4, 3, 1, 7, 6, 14, 2, 1, 7, 3, 4, 17, 2, 1, 3, 10, 5, 8, 24, 1, 6, 10, 1, 14, 18, 1, 17, 6, 12, 1, 3, 10, 5, 7, 1, 9, 2, 4, 11, 11, 18, 1, 5, 7, 1, 4, 1, 11, 6, 4, 12, 1, 6, 16, 1, 7, 10, 5, 3, 2, 1, 4, 8, 12, 1, 13, 4, 11, 11, 1, 5, 3, 1, 4, 1, 12, 4, 18, 1, 22, 1, 5, 3, 7, 1, 13, 9, 4, 21, 1, 11, 5, 24, 2, 1, 3, 10, 5, 7, 1, 3, 10, 4, 3, 1, 10, 4, 7, 1, 21, 2, 6, 21, 11, 2, 1, 12, 6, 20, 8, 11, 6, 4, 12, 5, 8, 17, 1, 5, 11, 11, 2, 17, 4, 11, 11, 18, 1, 3, 10, 2, 1, 3, 9, 4, 5, 11, 2, 9, 1, 11, 6, 6, 24, 7, 1, 11, 5, 24, 2, 1, 4, 1, 13, 6, 14, 21, 11, 2, 3, 2, 11, 18, 1, 12, 5, 16, 16, 2, 9, 2, 8, 3, 1, 16, 5, 11, 14, 1, 4, 3, 1, 11, 2, 4, 7, 3, 1, 5, 16, 1, 18, 6, 15, 1, 10, 4, 23, 2, 1, 12, 6, 20, 8, 11, 6, 4, 12, 1, 5, 3, 1, 18, 6, 15, 1, 10, 4, 23, 2, 8, 1, 3, 1, 20, 4, 7, 3, 2, 12, 1, 18, 6, 15, 9, 1, 3, 5, 14, 2, 1, 6, 9, 1, 14, 6, 8, 2, 18, 1, 12, 6, 8, 1, 3, 1, 20, 4, 7, 3, 2, 1, 18, 6, 15, 9, 1, 3, 5, 14, 2, 1, 3, 10, 5, 7, 1, 5, 7, 1, 21, 4, 5, 8, 16, 15, 11, 1, 22]\n1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e4a3e567bf3ab48fa2140c6c34f11b55f0323ef"
      },
      "cell_type": "code",
      "source": "# sort reviews by sequence length (no longer needed)\n# labels_tk = [l for r, l in sorted(zip(reviews_tk, labels_tk), key=lambda pair: -len(pair[0]))]\n# reviews_tk = sorted(reviews_tk, key=lambda r: -len(r))\n\n# for review, label in zip(reviews_tk[:15], labels_tk[:15]):\n#     print(len(review), label)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3645ad0a408e751d34b67cc7cd07348242b3d329"
      },
      "cell_type": "code",
      "source": "# now we padd the reviews\nmax_length = 2000\nreviews_lengths = []\n\nfor idx, review in enumerate(reviews_tk):\n    review = review[:max_length]\n    reviews_lengths.append(len(review))\n    review = ([0] * (max_length - len(review))) + review\n    reviews_tk[idx] = review\n    \nfor review, label, lengths in zip(reviews_tk[-15:], labels_tk[-15:], reviews_lengths[-15:]):\n    print(len(review), lengths, label)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "2000 272 1\n2000 783 0\n2000 1168 1\n2000 978 0\n2000 1432 1\n2000 636 0\n2000 1556 1\n2000 333 0\n2000 832 1\n2000 2000 0\n2000 1663 1\n2000 966 0\n2000 1242 1\n2000 929 0\n2000 736 1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "099e10d804765fa6d19ea45ccc2b41d7ad2d992d"
      },
      "cell_type": "code",
      "source": "# make training, validation and test splits\ntrain_count = 20000\nval_test_count = 2500\n\ntrain_x = np.array(reviews_tk[:train_count])\ntrain_y = np.array(labels_tk[:train_count])\n\nval_x = np.array(reviews_tk[train_count:train_count+val_test_count])\nval_y = np.array(labels_tk[train_count:train_count+val_test_count])\n\ntest_x = np.array(reviews_tk[-val_test_count:])\ntest_y = np.array(labels_tk[-val_test_count:])\n\nprint(train_x.shape, train_y.shape)\nprint(test_x.shape, test_y.shape)\nprint(val_x.shape, val_y.shape)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(20000, 2000) (20000,)\n(2500, 2000) (2500,)\n(2500, 2000) (2500,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83653737bd105c61f223f3552fa5b1faaba77453"
      },
      "cell_type": "code",
      "source": "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\nbatch_size=128\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd5ce9eed59b6a1a4a27484926ba9d821282b0fc"
      },
      "cell_type": "code",
      "source": "# let's now convert the tokenized lists to torch Tensors (no longer necessary)\n\n# reviews_tk = torch.from_numpy(np.array(reviews_tk))\n# labels_tk = torch.from_numpy(np.array(labels_tk))\n    \n# print(reviews_tk[-1][-10:]) # only prints the last 10 chars, but the tensor is very big, as big as the one shown above\n# print(labels_tk[-1])",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8250aa32ff44b1ae4f7675217a0cc8bd4dd1ba2f"
      },
      "cell_type": "code",
      "source": "# create model\nvocab_size = len(character_keys) + 1 # the + 1 is to account for the 0 that is used for padding\n\nclass SentimentModel(nn.Module):\n    def __init__(self):\n        super(SentimentModel, self).__init__()\n\n        self.embedding_layer = nn.Embedding(vocab_size, 32, padding_idx=0)\n        self.cnn1 = nn.Conv1d(32, 256, 3, padding=1, stride=2)\n        self.rnn = nn.GRU(256, 512, batch_first=True, num_layers=2, dropout=0.5)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(512, 2)\n        \n        self.hidden = None\n    \n    def forward(self, x):\n        embeddings = self.embedding_layer(x)\n        convolved = F.relu(self.cnn1(embeddings.transpose(1, 2)).transpose(2, 1))\n        # embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n        lstm_out, self.hidden = self.rnn(convolved, self.hidden)\n        # lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n        lstm_out = self.dropout(lstm_out)\n        out = self.fc(lstm_out)\n        out = out[:,-1,:]\n        out = torch.log_softmax(out, dim=1)\n        \n        return out",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f52e8a0d9b67fc1ba341df5a00d1c71ff51a7c1"
      },
      "cell_type": "code",
      "source": "model = SentimentModel()\nmodel",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "SentimentModel(\n  (embedding_layer): Embedding(29, 32, padding_idx=0)\n  (cnn1): Conv1d(32, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n  (rnn): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5)\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15ebf1e860673c0313c01a951c732ef1499de922"
      },
      "cell_type": "code",
      "source": "# let's train our model for 1 epoch\n\nepochs = 10\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.NLLLoss()\n\nmodel.cuda()\nfor e in range(1, epochs+1):\n    total_loss = 0\n    total_accuracy = 0\n    model.train()\n    batch = 0\n    for reviews, labels in train_loader:\n        batch += 1\n        reviews, labels = reviews.cuda(), labels.cuda()\n        \n        optimizer.zero_grad()\n        model.hidden = None\n        \n        pred = model(reviews)\n        loss = criterion(pred, labels)\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 4)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n        total_accuracy += torch.mean(equals.type(torch.FloatTensor))\n            \n        print(f\"EPOCH {e} ({batch}/{len(train_loader)}) - loss {total_loss/batch:.4f} - acc {total_accuracy/batch:.4f}\", end='\\r') \n    \n    valid_loss = 0\n    valid_accuracy = 0\n    with torch.no_grad():\n        model.eval()\n        for reviews, labels in valid_loader:\n            reviews, labels = reviews.cuda(), labels.cuda()\n            model.hidden = None\n            \n            pred = model(reviews)\n            loss = criterion(pred, labels)\n            \n            valid_loss += loss.item()\n            \n            equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n            valid_accuracy += torch.mean(equals.type(torch.FloatTensor))\n    \n    print(f\"EPOCH {e} - loss {total_loss/len(train_loader):.4f} - acc {total_accuracy/len(train_loader):.4f} - val_loss {valid_loss/len(valid_loader):.4f} - val_acc {valid_accuracy/len(valid_loader):.4f}\")\n    ",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "EPOCH 1 - loss 0.6942 - acc 0.5217 - val_loss 0.6795 - val_acc 0.5646\nEPOCH 2 - loss 0.6812 - acc 0.5609 - val_loss 0.6635 - val_acc 0.5774\nEPOCH 3 - loss 0.6710 - acc 0.5920 - val_loss 0.7016 - val_acc 0.5660\nEPOCH 4 - loss 0.5949 - acc 0.6811 - val_loss 0.5445 - val_acc 0.7243\nEPOCH 5 - loss 0.4712 - acc 0.7720 - val_loss 0.4320 - val_acc 0.7980\nEPOCH 6 - loss 0.4058 - acc 0.8128 - val_loss 0.4401 - val_acc 0.7950\nEPOCH 7 - loss 0.3625 - acc 0.8377 - val_loss 0.3889 - val_acc 0.8217\nEPOCH 8 - loss 0.3211 - acc 0.8629 - val_loss 0.4209 - val_acc 0.8069\nEPOCH 9 - loss 0.2719 - acc 0.8859 - val_loss 0.4194 - val_acc 0.8287\nEPOCH 10 - loss 0.2144 - acc 0.9137 - val_loss 0.4232 - val_acc 0.8153\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbf5501a1ba8506859a671c78dccaa2c1bea4e7f"
      },
      "cell_type": "code",
      "source": "test_loss = 0\ntest_accuracy = 0\nmodel.cuda()\nwith torch.no_grad():\n    model.eval()\n    for reviews, labels in test_loader:\n        reviews, labels = reviews.cuda(), labels.cuda()\n        model.hidden = None\n\n        pred = model(reviews)\n        loss = criterion(pred, labels)\n\n        test_loss += loss.item()\n\n        equals = torch.argmax(pred, dim=1).view(-1) == labels.view(-1)\n        test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n\nprint(f\"Final model -> loss: {test_loss / len(test_loader):.4f} - accuracy: {test_accuracy / len(test_loader):.4f}\")\n    ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Final model -> loss: 0.4226 - accuracy: 0.8255\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4e2013735cebc1740c3386e5c98e594462b0685"
      },
      "cell_type": "code",
      "source": "torch.save(model.state_dict(), 'model.pt')",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4de7397ece95160a99108d1e379d5af7c22e15df"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}